# Server configuration
server.port=8080

# Logging configuration
logging.level.root=INFO
logging.level.com.llmproxy=DEBUG

# Cache configuration
cache.enabled=true
cache.ttl.seconds=300
cache.max-items=1000
cache.eviction-policy=W-TinyLFU
# W-TinyLFU specific settings
cache.window-size-percentage=0.01
cache.sample-size=10
# LRU/LFU common settings
cache.initial-capacity=100
# Weight-based settings
cache.weight-based=true
cache.maximum-weight=100000000
cache.weigher=RESPONSE_SIZE
# Segment settings for concurrency
cache.concurrency-level=4
# Statistics collection
cache.record-stats=true
# Per-provider settings
cache.openai.ttl.seconds=600
cache.gemini.ttl.seconds=300
cache.mistral.ttl.seconds=450
cache.claude.ttl.seconds=500
# Access-based expiration
cache.expire-after-access=true
cache.access-expiration.seconds=1800

# Batch processing configuration (for multi-request grouping/dispatch)
batch.enabled=true
batch.max-size=16                # Default max batch size (requests) for all providers
batch.timeout.ms=50              # Default max wait time (ms) before dispatching non-full batch
batch.buffer.max-pending=128     # Default buffer size for pending requests per provider
batch.flush-on-overflow=true     # Flush batch when buffer is full, even before timeout

# Per-provider overrides (if omitted, global defaults apply)
batch.openai.enabled=true
batch.openai.max-size=8
batch.openai.timeout.ms=40
batch.openai.buffer.max-pending=64

batch.gemini.enabled=true
batch.gemini.max-size=16
batch.gemini.timeout.ms=60
batch.gemini.buffer.max-pending=128

batch.mistral.enabled=true
batch.mistral.max-size=10
batch.mistral.timeout.ms=50
batch.mistral.buffer.max-pending=80

batch.claude.enabled=true
batch.claude.max-size=12
batch.claude.timeout.ms=70
batch.claude.buffer.max-pending=96

# Retry Configuration
retry.max-attempts=3
retry.initial-backoff-ms=1000
retry.max-backoff-ms=30000
retry.backoff-multiplier=2.0
retry.jitter=0.1

# Rate limiting
rate-limit.requests-per-minute=60
rate-limit.burst=10

# API Keys (These should be set as environment variables in production)
api.openai.key=${OPENAI_API_KEY:}
api.gemini.key=${GEMINI_API_KEY:}
api.mistral.key=${MISTRAL_API_KEY:}
api.claude.key=${CLAUDE_API_KEY:}

# Global connection pool settings
connection.pool.default.max-total=100
connection.pool.default.max-per-route=20
connection.pool.default.connect-timeout-ms=5000
connection.pool.default.connection-request-timeout-ms=5000
connection.pool.default.socket-timeout-ms=30000
connection.pool.default.time-to-live-ms=300000
connection.pool.default.validate-after-inactivity-ms=10000
connection.pool.default.evict-idle-connections=true
connection.pool.default.idle-timeout-ms=60000

# OpenAI connection pool settings
connection.pool.openai.max-total=50
connection.pool.openai.max-per-route=20
connection.pool.openai.connect-timeout-ms=5000
connection.pool.openai.connection-request-timeout-ms=5000
connection.pool.openai.socket-timeout-ms=60000
connection.pool.openai.time-to-live-ms=300000

# Gemini connection pool settings
connection.pool.gemini.max-total=50
connection.pool.gemini.max-per-route=20
connection.pool.gemini.connect-timeout-ms=5000
connection.pool.gemini.connection-request-timeout-ms=5000
connection.pool.gemini.socket-timeout-ms=60000
connection.pool.gemini.time-to-live-ms=300000

# Mistral connection pool settings
connection.pool.mistral.max-total=50
connection.pool.mistral.max-per-route=20
connection.pool.mistral.connect-timeout-ms=5000
connection.pool.mistral.connection-request-timeout-ms=5000
connection.pool.mistral.socket-timeout-ms=45000
connection.pool.mistral.time-to-live-ms=300000

# Claude connection pool settings
connection.pool.claude.max-total=50
connection.pool.claude.max-per-route=20
connection.pool.claude.connect-timeout-ms=5000
connection.pool.claude.connection-request-timeout-ms=5000
connection.pool.claude.socket-timeout-ms=90000
connection.pool.claude.time-to-live-ms=300000